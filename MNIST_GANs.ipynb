{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_GANs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmiTR/Testprojekt-ET/blob/main/MNIST_GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation "
      ],
      "metadata": {
        "id": "BQEdGIEBof-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating connection with Google Drive"
      ],
      "metadata": {
        "id": "JqOFWg43oKUZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SHDumEpWixF",
        "outputId": "7462383e-a6c9-4db9-a916-907c55fddac7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set working directory to a folder in Google Drive to save all the inputs and outputs\n"
      ],
      "metadata": {
        "id": "ZlGqla6aqWba"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nzLpoC-Wxek",
        "outputId": "d1c19423-12fb-4cab-826f-faeeea5076d0"
      },
      "source": [
        "import os \n",
        "\n",
        "# the base Google Drive directory\n",
        "root_dir = \"/content/drive/My Drive/\"\n",
        "\n",
        "# choose path. Should create the founder beforehand \n",
        "project_folder = \"Colab Notebooks/MNIST GANs/\"\n",
        "\n",
        "def create_and_set_working_directory(project_folder):\n",
        "  # check if your project folder exists. if not, it will be created.\n",
        "  if os.path.isdir(root_dir + project_folder) == False:\n",
        "    os.mkdir(root_dir + project_folder)\n",
        "    print(root_dir + project_folder + ' did not exist but was created.')\n",
        "\n",
        "  #? can I just delete this path, knowing it hardly works \n",
        "\n",
        "  # change the OS to use your project folder as the working directory\n",
        "  os.chdir(root_dir + project_folder)\n",
        "\n",
        "  # create a test file to make sure it shows up in the right place\n",
        "  !touch 'new_file_in_working_directory.txt'\n",
        "  print('\\nYour working directory was changed to ' + root_dir + project_folder + \\\n",
        "        \"\\n\\nAn empty text file was created there. You can also run !pwd to confirm the current working directory.\" )\n",
        "\n",
        "create_and_set_working_directory(project_folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/MNIST GANs/ did not exist but was created.\n",
            "\n",
            "Your working directory was changed to /content/drive/My Drive/Colab Notebooks/MNIST GANs/\n",
            "\n",
            "An empty text file was created there. You can also run !pwd to confirm the current working directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q21VBby7XL-j",
        "outputId": "95890b1d-5e3b-458e-a4b2-6ddf696aeb8b"
      },
      "source": [
        "#check the directory\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/MNIST GANs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create folders"
      ],
      "metadata": {
        "id": "lbFB_OBUq3Wb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG4Ii5D7Wpy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adff1c87-00fb-4265-99ed-1e6a08499e50"
      },
      "source": [
        "!mkdir input\n",
        "!mkdir outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘input’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ymYBUGCasDim"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nqKLmvRJVnc3",
        "outputId": "6551a467-8fa9-4458-decd-9337ae4216e8"
      },
      "source": [
        "# Use this line to write Py model and save it exclusively in Drive -> \n",
        "%%writefile mnist_gan.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "# learning parameters\n",
        "batch_size = 512\n",
        "epochs = 200\n",
        "sample_size = 64 # fixed sample size\n",
        "nz = 128 # latent vector size\n",
        "k = 1 # number of steps to apply to the discriminator\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,),(0.5,)),\n",
        "])\n",
        "\n",
        "to_pil_image = transforms.ToPILImage()\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root='input/data',#\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nz = nz\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(self.nz, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x).view(-1, 1, 28, 28)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.n_input = 784\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(self.n_input, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        return self.main(x)\n",
        "\n",
        "generator = Generator(nz).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "print('##### GENERATOR #####')\n",
        "print(generator)\n",
        "print('######################')\n",
        "\n",
        "print('\\n##### DISCRIMINATOR #####')\n",
        "print(discriminator)\n",
        "print('######################')\n",
        "\n",
        "# optimizers\n",
        "optim_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
        "optim_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "\n",
        "# loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "losses_g = [] # to store generator loss after each epoch\n",
        "losses_d = [] # to store discriminator loss after each epoch\n",
        "images = [] # to store images generatd by the generator\n",
        "\n",
        "# to create real labels (1s)\n",
        "def label_real(size):\n",
        "    data = torch.ones(size, 1)\n",
        "    return data.to(device)\n",
        "\n",
        "# to create fake labels (0s)\n",
        "def label_fake(size):\n",
        "    data = torch.zeros(size, 1)\n",
        "    return data.to(device)\n",
        "\n",
        "# function to create the noise vector\n",
        "def create_noise(sample_size, nz):\n",
        "    return torch.randn(sample_size, nz).to(device)\n",
        "\n",
        "# to save the images generated by the generator\n",
        "def save_generator_image(image, path):\n",
        "    save_image(image, path)\n",
        "\n",
        "# function to train the discriminator network\n",
        "def train_discriminator(optimizer, data_real, data_fake):\n",
        "    b_size = data_real.size(0)\n",
        "    real_label = label_real(b_size)\n",
        "    fake_label = label_fake(b_size)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output_real = discriminator(data_real)\n",
        "    loss_real = criterion(output_real, real_label)\n",
        "\n",
        "    output_fake = discriminator(data_fake)\n",
        "    loss_fake = criterion(output_fake, fake_label)\n",
        "\n",
        "\n",
        "    loss_real.backward()\n",
        "    loss_fake.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss_real + loss_fake\n",
        "\n",
        "# function to train the generator network\n",
        "def train_generator(optimizer, data_fake):\n",
        "    b_size = data_fake.size(0)\n",
        "    real_label = label_real(b_size)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = discriminator(data_fake)\n",
        "    loss = criterion(output, real_label)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss    \n",
        "\n",
        "# create the noise vector\n",
        "noise = create_noise(sample_size, nz)\n",
        "\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_g = 0.0\n",
        "    loss_d = 0.0\n",
        "    for bi, data in tqdm(enumerate(train_loader), total=int(len(train_data)/train_loader.batch_size)):\n",
        "        image, _ = data\n",
        "        image = image.to(device)\n",
        "        b_size = len(image)\n",
        "        # run the discriminator for k number of steps\n",
        "        for step in range(k):\n",
        "            data_fake = generator(create_noise(b_size, nz)).detach()\n",
        "            data_real = image\n",
        "            # train the discriminator network\n",
        "            loss_d += train_discriminator(optim_d, data_real, data_fake)\n",
        "        data_fake = generator(create_noise(b_size, nz))\n",
        "        # train the generator network\n",
        "        loss_g += train_generator(optim_g, data_fake)\n",
        "\n",
        "    # create the final fake image for the epoch\n",
        "    generated_img = generator(noise).cpu().detach()\n",
        "    # make the images as grid\n",
        "    generated_img = make_grid(generated_img)\n",
        "    # save the generated torch tensor models to disk\n",
        "    save_generator_image(generated_img, f\"outputs/gen_img{epoch}.png\")\n",
        "    images.append(generated_img)\n",
        "    epoch_loss_g = loss_g / bi # total generator loss for the epoch\n",
        "    epoch_loss_d = loss_d / bi # total discriminator loss for the epoch\n",
        "    losses_g.append(epoch_loss_g)\n",
        "    losses_d.append(epoch_loss_d)\n",
        "    \n",
        "    print(f\"Epoch {epoch} of {epochs}\")\n",
        "    print(f\"Generator loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}\")\n",
        "\n",
        "print('DONE TRAINING')\n",
        "torch.save(generator.state_dict(), 'outputs/generator.pth')\n",
        "\n",
        "# save the generated images as GIF file\n",
        "imgs = [np.array(to_pil_image(img)) for img in images]\n",
        "imageio.mimsave('outputs/generator_images.gif', imgs)\n",
        "\n",
        "# plot and save the generator and discriminator loss\n",
        "plt.figure()\n",
        "plt.plot(losses_g, label='Generator loss')\n",
        "plt.plot(losses_d, label='Discriminator Loss')\n",
        "plt.legend()\n",
        "plt.savefig('outputs/loss.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### GENERATOR #####\n",
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.2)\n",
            "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Linear(in_features=1024, out_features=784, bias=True)\n",
            "    (7): Tanh()\n",
            "  )\n",
            ")\n",
            "######################\n",
            "\n",
            "##### DISCRIMINATOR #####\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Dropout(p=0.3, inplace=False)\n",
            "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Dropout(p=0.3, inplace=False)\n",
            "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n",
            "######################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "118it [00:17,  6.69it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 of 200\n",
            "Generator loss: 1.46968055, Discriminator loss: 0.87980038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "118it [00:17,  6.79it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 200\n",
            "Generator loss: 2.55979872, Discriminator loss: 1.21081996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "118it [00:17,  6.89it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 of 200\n",
            "Generator loss: 5.70614576, Discriminator loss: 0.21717748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 87/117 [00:12<00:04,  6.80it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b7f2d50da24b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# train the discriminator network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mloss_d\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdata_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;31m# train the generator network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mloss_g\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b7f2d50da24b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ojSHxARWzRR",
        "outputId": "be1f1640-217c-450b-f0f6-20f55e9b76e0"
      },
      "source": [
        "!python mnist_gan.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to input/data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "9913344it [00:00, 74009772.65it/s]                 \n",
            "Extracting input/data/MNIST/raw/train-images-idx3-ubyte.gz to input/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to input/data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "29696it [00:00, 73224016.22it/s]\n",
            "Extracting input/data/MNIST/raw/train-labels-idx1-ubyte.gz to input/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to input/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "1649664it [00:00, 46483391.76it/s]\n",
            "Extracting input/data/MNIST/raw/t10k-images-idx3-ubyte.gz to input/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to input/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "5120it [00:00, 33659618.31it/s]\n",
            "Extracting input/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to input/data/MNIST/raw\n",
            "\n",
            "##### GENERATOR #####\n",
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.2)\n",
            "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Linear(in_features=1024, out_features=784, bias=True)\n",
            "    (7): Tanh()\n",
            "  )\n",
            ")\n",
            "######################\n",
            "\n",
            "##### DISCRIMINATOR #####\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Dropout(p=0.3, inplace=False)\n",
            "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Dropout(p=0.3, inplace=False)\n",
            "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n",
            "######################\n",
            "118it [00:15,  7.65it/s]             \n",
            "Epoch 0 of 200\n",
            "Generator loss: 1.36386979, Discriminator loss: 0.91761118\n",
            "118it [00:15,  7.81it/s]             \n",
            "Epoch 1 of 200\n",
            "Generator loss: 2.68521523, Discriminator loss: 1.50399947\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 2 of 200\n",
            "Generator loss: 5.29672813, Discriminator loss: 0.23552737\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 3 of 200\n",
            "Generator loss: 2.35195971, Discriminator loss: 0.72249180\n",
            "118it [00:15,  7.68it/s]             \n",
            "Epoch 4 of 200\n",
            "Generator loss: 2.21659827, Discriminator loss: 1.05261028\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 5 of 200\n",
            "Generator loss: 1.91051149, Discriminator loss: 1.05251992\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 6 of 200\n",
            "Generator loss: 1.18733346, Discriminator loss: 1.22952867\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 7 of 200\n",
            "Generator loss: 2.27498674, Discriminator loss: 1.00350416\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 8 of 200\n",
            "Generator loss: 1.60539877, Discriminator loss: 1.08703780\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 9 of 200\n",
            "Generator loss: 1.55383742, Discriminator loss: 1.11022925\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 10 of 200\n",
            "Generator loss: 2.05680656, Discriminator loss: 1.09755826\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 11 of 200\n",
            "Generator loss: 2.06264734, Discriminator loss: 0.98474139\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 12 of 200\n",
            "Generator loss: 1.52778780, Discriminator loss: 1.15131533\n",
            "118it [00:15,  7.83it/s]             \n",
            "Epoch 13 of 200\n",
            "Generator loss: 1.25489163, Discriminator loss: 1.17433834\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 14 of 200\n",
            "Generator loss: 1.05858731, Discriminator loss: 1.19864225\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 15 of 200\n",
            "Generator loss: 1.72619319, Discriminator loss: 1.06466722\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 16 of 200\n",
            "Generator loss: 1.62780213, Discriminator loss: 1.09607160\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 17 of 200\n",
            "Generator loss: 2.42532945, Discriminator loss: 1.09960008\n",
            "118it [00:15,  7.67it/s]             \n",
            "Epoch 18 of 200\n",
            "Generator loss: 1.72359478, Discriminator loss: 1.01110303\n",
            "118it [00:15,  7.68it/s]             \n",
            "Epoch 19 of 200\n",
            "Generator loss: 2.37014723, Discriminator loss: 1.08466864\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 20 of 200\n",
            "Generator loss: 1.79817152, Discriminator loss: 1.07045865\n",
            "118it [00:15,  7.66it/s]             \n",
            "Epoch 21 of 200\n",
            "Generator loss: 1.19200158, Discriminator loss: 1.05322862\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 22 of 200\n",
            "Generator loss: 2.56065774, Discriminator loss: 0.94600654\n",
            "118it [00:15,  7.66it/s]             \n",
            "Epoch 23 of 200\n",
            "Generator loss: 1.37051010, Discriminator loss: 1.22521687\n",
            "118it [00:15,  7.72it/s]             \n",
            "Epoch 24 of 200\n",
            "Generator loss: 1.17877305, Discriminator loss: 1.13257408\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 25 of 200\n",
            "Generator loss: 1.08828783, Discriminator loss: 1.16362965\n",
            "118it [00:15,  7.83it/s]             \n",
            "Epoch 26 of 200\n",
            "Generator loss: 1.57666802, Discriminator loss: 1.04769146\n",
            "118it [00:15,  7.82it/s]             \n",
            "Epoch 27 of 200\n",
            "Generator loss: 2.25399446, Discriminator loss: 0.76423591\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 28 of 200\n",
            "Generator loss: 2.88683081, Discriminator loss: 0.68540949\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 29 of 200\n",
            "Generator loss: 2.74059391, Discriminator loss: 0.74358320\n",
            "118it [00:15,  7.70it/s]             \n",
            "Epoch 30 of 200\n",
            "Generator loss: 2.08256006, Discriminator loss: 0.80613679\n",
            "118it [00:15,  7.72it/s]             \n",
            "Epoch 31 of 200\n",
            "Generator loss: 1.88568830, Discriminator loss: 0.78071690\n",
            "118it [00:15,  7.71it/s]             \n",
            "Epoch 32 of 200\n",
            "Generator loss: 2.25099277, Discriminator loss: 0.74419755\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 33 of 200\n",
            "Generator loss: 1.95700836, Discriminator loss: 0.74085325\n",
            "118it [00:15,  7.67it/s]             \n",
            "Epoch 34 of 200\n",
            "Generator loss: 2.13907099, Discriminator loss: 0.70093501\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 35 of 200\n",
            "Generator loss: 2.82644033, Discriminator loss: 0.47150290\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 36 of 200\n",
            "Generator loss: 2.89317679, Discriminator loss: 0.57151359\n",
            "118it [00:15,  7.70it/s]             \n",
            "Epoch 37 of 200\n",
            "Generator loss: 3.17972493, Discriminator loss: 0.54015893\n",
            "118it [00:15,  7.71it/s]             \n",
            "Epoch 38 of 200\n",
            "Generator loss: 2.44774079, Discriminator loss: 0.56022978\n",
            "118it [00:15,  7.70it/s]             \n",
            "Epoch 39 of 200\n",
            "Generator loss: 2.60994864, Discriminator loss: 0.60510480\n",
            "118it [00:15,  7.67it/s]             \n",
            "Epoch 40 of 200\n",
            "Generator loss: 2.65439463, Discriminator loss: 0.58090264\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 41 of 200\n",
            "Generator loss: 3.00296617, Discriminator loss: 0.47972736\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 42 of 200\n",
            "Generator loss: 2.56062508, Discriminator loss: 0.59770089\n",
            "118it [00:15,  7.67it/s]             \n",
            "Epoch 43 of 200\n",
            "Generator loss: 2.64446306, Discriminator loss: 0.57681751\n",
            "118it [00:15,  7.72it/s]             \n",
            "Epoch 44 of 200\n",
            "Generator loss: 2.85661530, Discriminator loss: 0.58880162\n",
            "118it [00:15,  7.69it/s]             \n",
            "Epoch 45 of 200\n",
            "Generator loss: 2.92909956, Discriminator loss: 0.55374807\n",
            "118it [00:15,  7.69it/s]             \n",
            "Epoch 46 of 200\n",
            "Generator loss: 2.73666120, Discriminator loss: 0.59437096\n",
            "118it [00:15,  7.68it/s]             \n",
            "Epoch 47 of 200\n",
            "Generator loss: 2.80630541, Discriminator loss: 0.49946856\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 48 of 200\n",
            "Generator loss: 2.89858913, Discriminator loss: 0.50683296\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 49 of 200\n",
            "Generator loss: 3.02081084, Discriminator loss: 0.47226310\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 50 of 200\n",
            "Generator loss: 2.97789526, Discriminator loss: 0.45454541\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 51 of 200\n",
            "Generator loss: 3.28181314, Discriminator loss: 0.50477451\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 52 of 200\n",
            "Generator loss: 2.42622161, Discriminator loss: 0.63383675\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 53 of 200\n",
            "Generator loss: 2.97233295, Discriminator loss: 0.47840554\n",
            "118it [00:15,  7.72it/s]             \n",
            "Epoch 54 of 200\n",
            "Generator loss: 2.98392606, Discriminator loss: 0.47418910\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 55 of 200\n",
            "Generator loss: 3.06446171, Discriminator loss: 0.43242010\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 56 of 200\n",
            "Generator loss: 3.34806490, Discriminator loss: 0.41636327\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 57 of 200\n",
            "Generator loss: 3.38362885, Discriminator loss: 0.40513182\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 58 of 200\n",
            "Generator loss: 2.92003012, Discriminator loss: 0.53627050\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 59 of 200\n",
            "Generator loss: 2.90333295, Discriminator loss: 0.50865018\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 60 of 200\n",
            "Generator loss: 2.93306875, Discriminator loss: 0.50698215\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 61 of 200\n",
            "Generator loss: 3.08993626, Discriminator loss: 0.46669999\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 62 of 200\n",
            "Generator loss: 2.99281669, Discriminator loss: 0.51739907\n",
            "118it [00:15,  7.84it/s]             \n",
            "Epoch 63 of 200\n",
            "Generator loss: 3.07504916, Discriminator loss: 0.49484020\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 64 of 200\n",
            "Generator loss: 3.19847417, Discriminator loss: 0.46949130\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 65 of 200\n",
            "Generator loss: 2.92031026, Discriminator loss: 0.51246220\n",
            "118it [00:15,  7.79it/s]             \n",
            "Epoch 66 of 200\n",
            "Generator loss: 2.98944688, Discriminator loss: 0.47416687\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 67 of 200\n",
            "Generator loss: 2.89570951, Discriminator loss: 0.51121712\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 68 of 200\n",
            "Generator loss: 2.80347729, Discriminator loss: 0.53522331\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 69 of 200\n",
            "Generator loss: 3.06045961, Discriminator loss: 0.52432555\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 70 of 200\n",
            "Generator loss: 2.97706509, Discriminator loss: 0.50363356\n",
            "118it [00:15,  7.69it/s]             \n",
            "Epoch 71 of 200\n",
            "Generator loss: 2.94557762, Discriminator loss: 0.50613123\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 72 of 200\n",
            "Generator loss: 2.84108615, Discriminator loss: 0.49361631\n",
            "118it [00:15,  7.80it/s]             \n",
            "Epoch 73 of 200\n",
            "Generator loss: 2.69496775, Discriminator loss: 0.51127136\n",
            "118it [00:15,  7.72it/s]             \n",
            "Epoch 74 of 200\n",
            "Generator loss: 2.76272416, Discriminator loss: 0.53256261\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 75 of 200\n",
            "Generator loss: 2.69210029, Discriminator loss: 0.56474680\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 76 of 200\n",
            "Generator loss: 2.81593132, Discriminator loss: 0.52000326\n",
            "118it [00:15,  7.66it/s]             \n",
            "Epoch 77 of 200\n",
            "Generator loss: 2.52396941, Discriminator loss: 0.56654865\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 78 of 200\n",
            "Generator loss: 2.61936307, Discriminator loss: 0.53857237\n",
            "118it [00:15,  7.71it/s]             \n",
            "Epoch 79 of 200\n",
            "Generator loss: 2.65556312, Discriminator loss: 0.58079737\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 80 of 200\n",
            "Generator loss: 2.61383915, Discriminator loss: 0.57166618\n",
            "118it [00:15,  7.80it/s]             \n",
            "Epoch 81 of 200\n",
            "Generator loss: 2.56287074, Discriminator loss: 0.58745074\n",
            "118it [00:15,  7.82it/s]             \n",
            "Epoch 82 of 200\n",
            "Generator loss: 2.40576768, Discriminator loss: 0.62606949\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 83 of 200\n",
            "Generator loss: 2.58562541, Discriminator loss: 0.57983923\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 84 of 200\n",
            "Generator loss: 2.53643823, Discriminator loss: 0.59496462\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 85 of 200\n",
            "Generator loss: 2.47860265, Discriminator loss: 0.59372771\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 86 of 200\n",
            "Generator loss: 2.46118307, Discriminator loss: 0.58712393\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 87 of 200\n",
            "Generator loss: 2.57711744, Discriminator loss: 0.58943635\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 88 of 200\n",
            "Generator loss: 2.49556565, Discriminator loss: 0.60970205\n",
            "118it [00:15,  7.67it/s]             \n",
            "Epoch 89 of 200\n",
            "Generator loss: 2.41488814, Discriminator loss: 0.62417519\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 90 of 200\n",
            "Generator loss: 2.36677623, Discriminator loss: 0.66258806\n",
            "118it [00:15,  7.66it/s]             \n",
            "Epoch 91 of 200\n",
            "Generator loss: 2.22205377, Discriminator loss: 0.70047152\n",
            "118it [00:15,  7.68it/s]             \n",
            "Epoch 92 of 200\n",
            "Generator loss: 2.24629068, Discriminator loss: 0.68679160\n",
            "118it [00:15,  7.69it/s]             \n",
            "Epoch 93 of 200\n",
            "Generator loss: 2.20958591, Discriminator loss: 0.70950514\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 94 of 200\n",
            "Generator loss: 2.06083608, Discriminator loss: 0.73133528\n",
            "118it [00:15,  7.70it/s]             \n",
            "Epoch 95 of 200\n",
            "Generator loss: 2.08308625, Discriminator loss: 0.72879237\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 96 of 200\n",
            "Generator loss: 2.13167429, Discriminator loss: 0.70054412\n",
            "118it [00:15,  7.68it/s]             \n",
            "Epoch 97 of 200\n",
            "Generator loss: 2.04889965, Discriminator loss: 0.76683009\n",
            "118it [00:15,  7.82it/s]             \n",
            "Epoch 98 of 200\n",
            "Generator loss: 1.97784841, Discriminator loss: 0.78228390\n",
            "118it [00:15,  7.65it/s]             \n",
            "Epoch 99 of 200\n",
            "Generator loss: 2.11340284, Discriminator loss: 0.73282009\n",
            "118it [00:15,  7.81it/s]             \n",
            "Epoch 100 of 200\n",
            "Generator loss: 2.07824898, Discriminator loss: 0.76511949\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 101 of 200\n",
            "Generator loss: 1.98689604, Discriminator loss: 0.78562492\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 102 of 200\n",
            "Generator loss: 1.98525953, Discriminator loss: 0.77561772\n",
            "118it [00:15,  7.71it/s]             \n",
            "Epoch 103 of 200\n",
            "Generator loss: 1.97475278, Discriminator loss: 0.77475291\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 104 of 200\n",
            "Generator loss: 1.93263519, Discriminator loss: 0.80613804\n",
            "118it [00:15,  7.81it/s]             \n",
            "Epoch 105 of 200\n",
            "Generator loss: 1.92876637, Discriminator loss: 0.80507660\n",
            "118it [00:15,  7.80it/s]             \n",
            "Epoch 106 of 200\n",
            "Generator loss: 1.82807386, Discriminator loss: 0.83195162\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 107 of 200\n",
            "Generator loss: 1.79641271, Discriminator loss: 0.84452027\n",
            "118it [00:15,  7.83it/s]             \n",
            "Epoch 108 of 200\n",
            "Generator loss: 1.80527854, Discriminator loss: 0.85733557\n",
            "118it [00:15,  7.79it/s]             \n",
            "Epoch 109 of 200\n",
            "Generator loss: 1.84090483, Discriminator loss: 0.84476435\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 110 of 200\n",
            "Generator loss: 1.81771946, Discriminator loss: 0.83538377\n",
            "118it [00:15,  7.70it/s]             \n",
            "Epoch 111 of 200\n",
            "Generator loss: 1.79217744, Discriminator loss: 0.86461920\n",
            "118it [00:15,  7.83it/s]             \n",
            "Epoch 112 of 200\n",
            "Generator loss: 1.84270620, Discriminator loss: 0.86839783\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 113 of 200\n",
            "Generator loss: 1.76731253, Discriminator loss: 0.85212874\n",
            "118it [00:15,  7.66it/s]             \n",
            "Epoch 114 of 200\n",
            "Generator loss: 1.76234865, Discriminator loss: 0.86697221\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 115 of 200\n",
            "Generator loss: 1.83588266, Discriminator loss: 0.83734584\n",
            "118it [00:15,  7.67it/s]             \n",
            "Epoch 116 of 200\n",
            "Generator loss: 1.79210031, Discriminator loss: 0.87297934\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 117 of 200\n",
            "Generator loss: 1.75457299, Discriminator loss: 0.87410474\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 118 of 200\n",
            "Generator loss: 1.80598772, Discriminator loss: 0.84718746\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 119 of 200\n",
            "Generator loss: 1.76717436, Discriminator loss: 0.84805733\n",
            "118it [00:15,  7.80it/s]             \n",
            "Epoch 120 of 200\n",
            "Generator loss: 1.85234797, Discriminator loss: 0.83718121\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 121 of 200\n",
            "Generator loss: 1.78243673, Discriminator loss: 0.88415188\n",
            "118it [00:15,  7.71it/s]             \n",
            "Epoch 122 of 200\n",
            "Generator loss: 1.74376595, Discriminator loss: 0.87747657\n",
            "118it [00:15,  7.70it/s]             \n",
            "Epoch 123 of 200\n",
            "Generator loss: 1.69269252, Discriminator loss: 0.91635215\n",
            "118it [00:15,  7.72it/s]             \n",
            "Epoch 124 of 200\n",
            "Generator loss: 1.70709193, Discriminator loss: 0.88412648\n",
            "118it [00:15,  7.70it/s]             \n",
            "Epoch 125 of 200\n",
            "Generator loss: 1.73417544, Discriminator loss: 0.91785681\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 126 of 200\n",
            "Generator loss: 1.73942554, Discriminator loss: 0.89121825\n",
            "118it [00:15,  7.62it/s]             \n",
            "Epoch 127 of 200\n",
            "Generator loss: 1.76020586, Discriminator loss: 0.85859013\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 128 of 200\n",
            "Generator loss: 1.83966720, Discriminator loss: 0.84709716\n",
            "118it [00:15,  7.69it/s]             \n",
            "Epoch 129 of 200\n",
            "Generator loss: 1.68569493, Discriminator loss: 0.91225374\n",
            "118it [00:15,  7.67it/s]             \n",
            "Epoch 130 of 200\n",
            "Generator loss: 1.68017793, Discriminator loss: 0.90763754\n",
            "118it [00:15,  7.69it/s]             \n",
            "Epoch 131 of 200\n",
            "Generator loss: 1.67361331, Discriminator loss: 0.89484513\n",
            "118it [00:15,  7.70it/s]             \n",
            "Epoch 132 of 200\n",
            "Generator loss: 1.70629501, Discriminator loss: 0.88689631\n",
            "118it [00:15,  7.69it/s]             \n",
            "Epoch 133 of 200\n",
            "Generator loss: 1.67574406, Discriminator loss: 0.91740686\n",
            "118it [00:15,  7.66it/s]             \n",
            "Epoch 134 of 200\n",
            "Generator loss: 1.57766306, Discriminator loss: 0.92604566\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 135 of 200\n",
            "Generator loss: 1.66839206, Discriminator loss: 0.91705328\n",
            "118it [00:15,  7.69it/s]             \n",
            "Epoch 136 of 200\n",
            "Generator loss: 1.64691961, Discriminator loss: 0.93487608\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 137 of 200\n",
            "Generator loss: 1.54498351, Discriminator loss: 0.94683778\n",
            "118it [00:15,  7.71it/s]             \n",
            "Epoch 138 of 200\n",
            "Generator loss: 1.61550999, Discriminator loss: 0.93453413\n",
            "118it [00:15,  7.68it/s]             \n",
            "Epoch 139 of 200\n",
            "Generator loss: 1.55519462, Discriminator loss: 0.94837826\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 140 of 200\n",
            "Generator loss: 1.57712293, Discriminator loss: 0.95102358\n",
            "118it [00:15,  7.70it/s]             \n",
            "Epoch 141 of 200\n",
            "Generator loss: 1.60438609, Discriminator loss: 0.93289900\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 142 of 200\n",
            "Generator loss: 1.62354755, Discriminator loss: 0.92117083\n",
            "118it [00:15,  7.79it/s]             \n",
            "Epoch 143 of 200\n",
            "Generator loss: 1.58970690, Discriminator loss: 0.94552439\n",
            "118it [00:15,  7.80it/s]             \n",
            "Epoch 144 of 200\n",
            "Generator loss: 1.59851098, Discriminator loss: 0.94062686\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 145 of 200\n",
            "Generator loss: 1.61412132, Discriminator loss: 0.96113330\n",
            "118it [00:15,  7.69it/s]             \n",
            "Epoch 146 of 200\n",
            "Generator loss: 1.54456758, Discriminator loss: 0.94778681\n",
            "118it [00:15,  7.68it/s]             \n",
            "Epoch 147 of 200\n",
            "Generator loss: 1.56374061, Discriminator loss: 0.96740037\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 148 of 200\n",
            "Generator loss: 1.49849403, Discriminator loss: 0.96433717\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 149 of 200\n",
            "Generator loss: 1.49266899, Discriminator loss: 0.98576367\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 150 of 200\n",
            "Generator loss: 1.49240220, Discriminator loss: 0.98942047\n",
            "118it [00:15,  7.65it/s]             \n",
            "Epoch 151 of 200\n",
            "Generator loss: 1.49615693, Discriminator loss: 0.98310733\n",
            "118it [00:15,  7.71it/s]             \n",
            "Epoch 152 of 200\n",
            "Generator loss: 1.48957479, Discriminator loss: 0.97011465\n",
            "118it [00:15,  7.66it/s]             \n",
            "Epoch 153 of 200\n",
            "Generator loss: 1.47613883, Discriminator loss: 0.98822945\n",
            "118it [00:15,  7.68it/s]             \n",
            "Epoch 154 of 200\n",
            "Generator loss: 1.48059130, Discriminator loss: 0.98921305\n",
            "118it [00:15,  7.77it/s]             \n",
            "Epoch 155 of 200\n",
            "Generator loss: 1.46521318, Discriminator loss: 1.00328398\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 156 of 200\n",
            "Generator loss: 1.41873729, Discriminator loss: 1.01033986\n",
            "118it [00:15,  7.73it/s]             \n",
            "Epoch 157 of 200\n",
            "Generator loss: 1.49762499, Discriminator loss: 0.99640679\n",
            "118it [00:15,  7.80it/s]             \n",
            "Epoch 158 of 200\n",
            "Generator loss: 1.44970107, Discriminator loss: 1.00292611\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 159 of 200\n",
            "Generator loss: 1.44977045, Discriminator loss: 0.99615979\n",
            "118it [00:15,  7.65it/s]             \n",
            "Epoch 160 of 200\n",
            "Generator loss: 1.43690872, Discriminator loss: 1.01674306\n",
            "118it [00:15,  7.72it/s]             \n",
            "Epoch 161 of 200\n",
            "Generator loss: 1.44416893, Discriminator loss: 0.99463576\n",
            "118it [00:15,  7.68it/s]             \n",
            "Epoch 162 of 200\n",
            "Generator loss: 1.41188824, Discriminator loss: 1.01218855\n",
            "118it [00:15,  7.71it/s]             \n",
            "Epoch 163 of 200\n",
            "Generator loss: 1.46063757, Discriminator loss: 0.99924582\n",
            "118it [00:15,  7.81it/s]             \n",
            "Epoch 164 of 200\n",
            "Generator loss: 1.44081318, Discriminator loss: 1.01147830\n",
            "118it [00:15,  7.79it/s]             \n",
            "Epoch 165 of 200\n",
            "Generator loss: 1.45705628, Discriminator loss: 0.98952717\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 166 of 200\n",
            "Generator loss: 1.37837791, Discriminator loss: 1.04280221\n",
            "118it [00:15,  7.72it/s]             \n",
            "Epoch 167 of 200\n",
            "Generator loss: 1.40641618, Discriminator loss: 1.02962101\n",
            "118it [00:15,  7.83it/s]             \n",
            "Epoch 168 of 200\n",
            "Generator loss: 1.37555039, Discriminator loss: 1.02185953\n",
            "118it [00:15,  7.80it/s]             \n",
            "Epoch 169 of 200\n",
            "Generator loss: 1.43884182, Discriminator loss: 1.02618551\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 170 of 200\n",
            "Generator loss: 1.39266741, Discriminator loss: 1.03720427\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 171 of 200\n",
            "Generator loss: 1.41254246, Discriminator loss: 1.02597022\n",
            "118it [00:15,  7.78it/s]             \n",
            "Epoch 172 of 200\n",
            "Generator loss: 1.44056821, Discriminator loss: 1.00413251\n",
            "118it [00:15,  7.68it/s]             \n",
            "Epoch 173 of 200\n",
            "Generator loss: 1.44726086, Discriminator loss: 1.01821589\n",
            "118it [00:15,  7.79it/s]             \n",
            "Epoch 174 of 200\n",
            "Generator loss: 1.36908686, Discriminator loss: 1.02134347\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 175 of 200\n",
            "Generator loss: 1.42307973, Discriminator loss: 1.01599753\n",
            "118it [00:15,  7.79it/s]             \n",
            "Epoch 176 of 200\n",
            "Generator loss: 1.38040066, Discriminator loss: 1.03754663\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 177 of 200\n",
            "Generator loss: 1.33128440, Discriminator loss: 1.05091906\n",
            "118it [00:15,  7.80it/s]             \n",
            "Epoch 178 of 200\n",
            "Generator loss: 1.32930362, Discriminator loss: 1.04890633\n",
            "118it [00:15,  7.72it/s]             \n",
            "Epoch 179 of 200\n",
            "Generator loss: 1.35244572, Discriminator loss: 1.03731608\n",
            "118it [00:15,  7.74it/s]             \n",
            "Epoch 180 of 200\n",
            "Generator loss: 1.35243869, Discriminator loss: 1.04777193\n",
            "118it [00:15,  7.71it/s]             \n",
            "Epoch 181 of 200\n",
            "Generator loss: 1.33807635, Discriminator loss: 1.05914521\n",
            "118it [00:15,  7.76it/s]             \n",
            "Epoch 182 of 200\n",
            "Generator loss: 1.36008739, Discriminator loss: 1.03790736\n",
            "118it [00:15,  7.64it/s]             \n",
            "Epoch 183 of 200\n",
            "Generator loss: 1.35504496, Discriminator loss: 1.05278230\n",
            "118it [00:15,  7.67it/s]             \n",
            "Epoch 184 of 200\n",
            "Generator loss: 1.34987950, Discriminator loss: 1.04726064\n",
            "118it [00:15,  7.63it/s]             \n",
            "Epoch 185 of 200\n",
            "Generator loss: 1.29331994, Discriminator loss: 1.07199442\n",
            "118it [00:15,  7.68it/s]             \n",
            "Epoch 186 of 200\n",
            "Generator loss: 1.33611059, Discriminator loss: 1.05722368\n",
            "118it [00:15,  7.71it/s]             \n",
            "Epoch 187 of 200\n",
            "Generator loss: 1.35688305, Discriminator loss: 1.06332195\n",
            "118it [00:15,  7.72it/s]             \n",
            "Epoch 188 of 200\n",
            "Generator loss: 1.33186543, Discriminator loss: 1.06002414\n",
            "118it [00:15,  7.67it/s]             \n",
            "Epoch 189 of 200\n",
            "Generator loss: 1.39941323, Discriminator loss: 1.03280115\n",
            "118it [00:15,  7.66it/s]             \n",
            "Epoch 190 of 200\n",
            "Generator loss: 1.30323911, Discriminator loss: 1.07216311\n",
            "118it [00:15,  7.75it/s]             \n",
            "Epoch 191 of 200\n",
            "Generator loss: 1.32959533, Discriminator loss: 1.06604993\n",
            "118it [00:15,  7.70it/s]             \n",
            "Epoch 192 of 200\n",
            "Generator loss: 1.31593931, Discriminator loss: 1.06308460\n",
            "118it [00:15,  7.71it/s]             \n",
            "Epoch 193 of 200\n",
            "Generator loss: 1.34444678, Discriminator loss: 1.05651736\n",
            "118it [00:15,  7.59it/s]             \n",
            "Epoch 194 of 200\n",
            "Generator loss: 1.29874885, Discriminator loss: 1.07903385\n",
            "118it [00:15,  7.70it/s]             \n",
            "Epoch 195 of 200\n",
            "Generator loss: 1.29067945, Discriminator loss: 1.05542910\n",
            "118it [00:15,  7.69it/s]             \n",
            "Epoch 196 of 200\n",
            "Generator loss: 1.34344554, Discriminator loss: 1.05520487\n",
            "118it [00:15,  7.64it/s]             \n",
            "Epoch 197 of 200\n",
            "Generator loss: 1.31448913, Discriminator loss: 1.08050859\n",
            "118it [00:15,  7.65it/s]             \n",
            "Epoch 198 of 200\n",
            "Generator loss: 1.32911861, Discriminator loss: 1.06514776\n",
            "118it [00:15,  7.67it/s]             \n",
            "Epoch 199 of 200\n",
            "Generator loss: 1.28985393, Discriminator loss: 1.07739437\n",
            "DONE TRAINING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYJgc9K4W03I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}